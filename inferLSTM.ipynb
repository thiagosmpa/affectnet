{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import function as fn\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_center(image, target_shape):\n",
    "    h, w = image.shape[:2]\n",
    "    target_w, target_h = target_shape['shape']\n",
    "    target_x, target_y = target_shape['position']\n",
    "\n",
    "    if not target_shape['target'] == 'face':\n",
    "        orientation = 'vertical' if h > w else 'horizontal'\n",
    "\n",
    "        if orientation == 'horizontal':\n",
    "            image = imutils.resize(image, width=target_w)\n",
    "            y = target_y + (target_h - image.shape[0]) // 2\n",
    "            x = target_x\n",
    "        \n",
    "        else:\n",
    "            image = imutils.resize(image, height=target_h)\n",
    "            x = target_x + (target_w - image.shape[1]) // 2\n",
    "            y = target_y\n",
    "        \n",
    "        return image, (x, y)\n",
    "    else:\n",
    "        image = cv2.resize(image, (target_w, target_h))\n",
    "        x = target_x\n",
    "        y = target_y\n",
    "        return image, (x, y)\n",
    "\n",
    "def draw_text_on_screen(screen, text, position, font_size=20, color=(0, 0, 0)):\n",
    "    screen_pil = Image.fromarray(cv2.cvtColor(screen, cv2.COLOR_BGR2RGB))\n",
    "    font = ImageFont.truetype(\"src/Bebas.ttf\", font_size)\n",
    "    draw = ImageDraw.Draw(screen_pil)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(screen_pil), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def place_image_on_screen(screen, cv2_image, target_shape):\n",
    "    resized_image, (x, y) = resize_and_center(cv2_image, target_shape)\n",
    "    screen[y:y + resized_image.shape[0], x:x + resized_image.shape[1]] = resized_image\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = (1280, 800)\n",
    "\n",
    "right_offset = (0,70)\n",
    "left_offset = (0,-20)\n",
    "\n",
    "video_shape = {'shape': (537, 570), 'position': (85, 155 + left_offset[1]), 'target': 'video'}\n",
    "graphic_shape = {'shape': (537, 398), 'position': (658, 290 + right_offset[1]), 'target': 'graphic'}\n",
    "face_shape = {'shape': (200, 200), 'position': (728, 105 + right_offset[1]), 'target': 'face'}\n",
    "predominant_shape = {'shape': (252, 298), 'position': (955, 85 + right_offset[1]), 'target': 'predominant'}\n",
    "\n",
    "def update_screen_info(screen, predominant_emotion, emotion_duration, current_emotion, current_emotion_prob, backbone_model, LSTM_model, device, fps):\n",
    "    \n",
    "    text_predominant_emotion = f'Predominant Emotion: {predominant_emotion}'\n",
    "    text_duration = f'Duration: {emotion_duration:.2f} seconds'\n",
    "    text_current_emotion = f'Emotion: {current_emotion} ({current_emotion_prob*100:.2f}%)'\n",
    "    \n",
    "    screen = draw_text_on_screen(screen, text_current_emotion, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 140), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, text_duration, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 170), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, text_predominant_emotion, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 200), font_size=20, color=(0, 0, 0))\n",
    "    \n",
    "    screen = draw_text_on_screen(screen, 'EMOTION CLASSIFICATION', (450, 30), 40, (0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'Backbone: {backbone_model}', (video_shape['position'][0], video_shape['position'][1] - 60), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'LSTM: {LSTM_model}', (video_shape['position'][0], video_shape['position'][1] - 35), font_size=20, color=(0, 0, 0))\n",
    "\n",
    "    screen = draw_text_on_screen(screen, f'Using {device}', (video_shape['position'][0], video_shape['position'][1] - 85), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'FPS: {fps:.1f}', (video_shape['position'][0], video_shape['position'][1] - 110), font_size=20, color=(0, 0, 0))\n",
    "    \n",
    "    return screen\n",
    "\n",
    "    \n",
    "def update_emotion_graph(ax, emotion_probs):\n",
    "    ax.clear()\n",
    "    for emotion, probs in emotion_probs.items():\n",
    "        if probs:\n",
    "            ax.plot(probs, label=emotion)\n",
    "            ax.annotate(emotion, \n",
    "                        xy=(len(probs) - 1, probs[-1]), \n",
    "                        xytext=(5, 0), \n",
    "                        textcoords='offset points',\n",
    "                        color=ax.get_lines()[-1].get_color(),\n",
    "                        fontsize=8,\n",
    "                        fontweight='regular')\n",
    "    \n",
    "    ax.set_xlabel('Frames')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title('Emotions Probabilities')\n",
    "    ax.legend(fontsize='x-small', loc='upper left')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path, backbone_model='0_66_49_wo_gl', LSTM_model='RAVDESS', device=device):\n",
    "\n",
    "    pth_backbone_model = torch.jit.load(f'model/Torch/torchscript_model_{backbone_model}.pth').to(device)\n",
    "    pth_backbone_model.eval()\n",
    "\n",
    "    pth_LSTM_model = torch.jit.load(f'model/Torch/{LSTM_model}.pth').to(device)\n",
    "    pth_LSTM_model.eval()\n",
    "\n",
    "    DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "    emotion_probs = {emotion: [] for emotion in DICT_EMO.values()}\n",
    "    \n",
    "    output_path_basename = os.path.basename(output_path)\n",
    "    output_log_path = f'logs/{output_path_basename}.csv'\n",
    "\n",
    "    data_list = []\n",
    "    emotion_history = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    combined_writer = cv2.VideoWriter(output_path, fourcc, fps, (screen_shape[0], screen_shape[1]))\n",
    "    \n",
    "    blank_screen = np.ones((screen_shape[1], screen_shape[0], 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    lstm_features = []\n",
    "    emotion_probs = {emotion: [] for emotion in DICT_EMO.values()}\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5.37, 2.95))\n",
    "    \n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    current_emotion = None\n",
    "    emotion_start_time = None\n",
    "    \n",
    "    try:\n",
    "        for frame_count in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "            t1 = time.time()\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            screen = blank_screen.copy()\n",
    "            \n",
    "            frame_copy = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_copy)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                for fl in results.multi_face_landmarks:\n",
    "                    startX, startY, endX, endY = fn.get_box(fl, w, h)\n",
    "                    face_copy = frame_copy[startY:endY, startX:endX]\n",
    "                    face_copy = cv2.cvtColor(face_copy, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    cur_face = fn.pth_processing(Image.fromarray(frame_copy[startY:endY, startX:endX]))\n",
    "                    features = torch.nn.functional.relu(pth_backbone_model.extract_features(cur_face)).cpu().detach().numpy()\n",
    "                    \n",
    "                    if not lstm_features:\n",
    "                        lstm_features = [features] * 10\n",
    "                    else:\n",
    "                        lstm_features.pop(0)\n",
    "                        lstm_features.append(features)\n",
    "                    \n",
    "                    lstm_f = torch.unsqueeze(torch.from_numpy(np.vstack(lstm_features)), 0).to(device)\n",
    "                    output = pth_LSTM_model(lstm_f).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i, emotion in DICT_EMO.items():\n",
    "                        emotion_probs[emotion].append(output[0, i])\n",
    "                    \n",
    "                    cl = np.argmax(output)\n",
    "                    label = DICT_EMO[cl]\n",
    "                    frame = fn.annotate(frame, (startX, startY, endX, endY), label)\n",
    "                    \n",
    "                    face_region = face_copy\n",
    "                    screen = place_image_on_screen(screen, face_region, face_shape)\n",
    "                    \n",
    "                    if label != current_emotion:\n",
    "                        current_emotion = label\n",
    "                        emotion_start_time = time.time()\n",
    "                    \n",
    "                    if emotion_start_time is not None:\n",
    "                        emotion_duration = time.time() - emotion_start_time\n",
    "                    else:\n",
    "                        emotion_duration = 0\n",
    "                    \n",
    "                    emotion_history.append(label)\n",
    "                    \n",
    "                    predominant_emotion = max(set(emotion_history), key=emotion_history.count)\n",
    "                    current_emotion_prob = output[0, cl]\n",
    "                    \n",
    "                    data_list.append({\n",
    "                        'Frame': frame_count,\n",
    "                        'Emotion': current_emotion,\n",
    "                        'Time': emotion_duration,\n",
    "                        'NE_PROB': output[0, 0],\n",
    "                        'HA_PROB': output[0, 1],\n",
    "                        'SA_PROB': output[0, 2],\n",
    "                        'SU_PROB': output[0, 3],\n",
    "                        'FE_PROB': output[0, 4],\n",
    "                        'DI_PROB': output[0, 5],\n",
    "                        'AN_PROB': output[0, 6]\n",
    "                    })\n",
    "\n",
    "            screen = place_image_on_screen(screen, frame, video_shape)\n",
    "            \n",
    "            update_emotion_graph(ax, emotion_probs)\n",
    "            \n",
    "            fig.canvas.draw()\n",
    "            plot_frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            plot_frame = plot_frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plot_frame = cv2.cvtColor(plot_frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            screen = place_image_on_screen(screen, plot_frame, graphic_shape)\n",
    "            \n",
    "            fps = 1 / (time.time() - t1)\n",
    "            screen = update_screen_info(screen, predominant_emotion, emotion_duration, current_emotion, current_emotion_prob, backbone_model, LSTM_model, device, fps)\n",
    "            \n",
    "            combined_writer.write(screen)\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        combined_writer.release()\n",
    "        face_mesh.close()\n",
    "        plt.close(fig)\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        df.to_csv(output_log_path, index=False)\n",
    "    \n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_available = {\n",
    "    'backbone': ['0_66_37_wo_gl', '0_66_49_wo_gl'],\n",
    "    'LSTM': ['RAVDESS', 'CREMA-D', 'Aff-Wild2', 'SAVEE', 'RAMAS', 'IEMOCAP']\n",
    "}\n",
    "\n",
    "backbone_model = '0_66_49_wo_gl'\n",
    "LSTM_model = 'SAVEE'\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "video_src = 'src/will.mp4'\n",
    "video_name = os.path.basename(video_src).split('.')[0]\n",
    "output_path = f'output/{backbone_model}_{LSTM_model}_{video_name}.mp4'\n",
    "final_screen = process_video(\n",
    "    video_path=video_src, \n",
    "    output_path=output_path, \n",
    "    backbone_model=backbone_model, \n",
    "    LSTM_model=LSTM_model, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(cv2.cvtColor(final_screen, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'Output video saved at {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
