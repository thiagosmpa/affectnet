{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "import function as fn\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def init(self):\n",
    "            super(PreprocessInput, self).init()\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0).to(device)\n",
    "        return img\n",
    "    return get_img_torch(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNOTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_center(image, target_shape):\n",
    "    h, w = image.shape[:2]\n",
    "    target_w, target_h = target_shape['shape']\n",
    "    target_x, target_y = target_shape['position']\n",
    "\n",
    "    if not target_shape['target'] == 'face':\n",
    "        orientation = 'vertical' if h > w else 'horizontal'\n",
    "\n",
    "        if orientation == 'horizontal':\n",
    "            image = imutils.resize(image, width=target_w)\n",
    "            y = target_y + (target_h - image.shape[0]) // 2\n",
    "            x = target_x\n",
    "        \n",
    "        else:\n",
    "            image = imutils.resize(image, height=target_h)\n",
    "            x = target_x + (target_w - image.shape[1]) // 2\n",
    "            y = target_y\n",
    "        \n",
    "        return image, (x, y)\n",
    "    else:\n",
    "        image = cv2.resize(image, (target_w, target_h))\n",
    "        x = target_x\n",
    "        y = target_y\n",
    "        return image, (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text_on_screen(screen, text, position, font_size=20, color=(0, 0, 0)):\n",
    "    screen_pil = Image.fromarray(cv2.cvtColor(screen, cv2.COLOR_BGR2RGB))\n",
    "    font = ImageFont.truetype(\"src/Bebas.ttf\", font_size)\n",
    "    draw = ImageDraw.Draw(screen_pil)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(screen_pil), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_image_on_screen(screen, cv2_image, target_shape):\n",
    "    resized_image, (x, y) = resize_and_center(cv2_image, target_shape)\n",
    "    screen[y:y + resized_image.shape[0], x:x + resized_image.shape[1]] = resized_image\n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_shape = (1280, 800)\n",
    "\n",
    "right_offset = (0,70)\n",
    "left_offset = (0,-20)\n",
    "\n",
    "video_shape = {'shape': (537, 570), 'position': (85, 155 + left_offset[1]), 'target': 'video'}\n",
    "graphic_shape = {'shape': (537, 398), 'position': (658, 290 + right_offset[1]), 'target': 'graphic'}\n",
    "face_shape = {'shape': (200, 200), 'position': (728, 105 + right_offset[1]), 'target': 'face'}\n",
    "predominant_shape = {'shape': (252, 298), 'position': (955, 85 + right_offset[1]), 'target': 'predominant'}\n",
    "\n",
    "def update_screen_info(screen, predominant_emotion, emotion_duration, current_emotion, current_emotion_prob, backbone_model, LSTM_model, device, fps):\n",
    "    \n",
    "    text_predominant_emotion = f'Predominant Emotion: {predominant_emotion}'\n",
    "    text_duration = f'Duration: {emotion_duration:.2f} seconds'\n",
    "    text_current_emotion = f'Emotion: {current_emotion} ({current_emotion_prob*100:.2f}%)'\n",
    "    \n",
    "    screen = draw_text_on_screen(screen, text_current_emotion, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 140), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, text_duration, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 170), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, text_predominant_emotion, (predominant_shape['position'][0] + 10, predominant_shape['position'][1] + 200), font_size=20, color=(0, 0, 0))\n",
    "    \n",
    "    screen = draw_text_on_screen(screen, 'EMOTION CLASSIFICATION', (450, 30), 40, (0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'Backbone: {backbone_model}', (video_shape['position'][0], video_shape['position'][1] - 60), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'LSTM: {LSTM_model}', (video_shape['position'][0], video_shape['position'][1] - 35), font_size=20, color=(0, 0, 0))\n",
    "\n",
    "    screen = draw_text_on_screen(screen, f'Using {device}', (video_shape['position'][0], video_shape['position'][1] - 85), font_size=20, color=(0, 0, 0))\n",
    "    screen = draw_text_on_screen(screen, f'FPS: {fps:.1f}', (video_shape['position'][0], video_shape['position'][1] - 110), font_size=20, color=(0, 0, 0))\n",
    "    \n",
    "    return screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_emotion_graph(ax, emotion_probs):\n",
    "    ax.clear()\n",
    "    for emotion, probs in emotion_probs.items():\n",
    "        if probs:\n",
    "            ax.plot(probs, label=emotion)\n",
    "            ax.annotate(emotion, \n",
    "                        xy=(len(probs) - 1, probs[-1]), \n",
    "                        xytext=(5, 0), \n",
    "                        textcoords='offset points',\n",
    "                        color=ax.get_lines()[-1].get_color(),\n",
    "                        fontsize=8,\n",
    "                        fontweight='regular')\n",
    "    \n",
    "    ax.set_xlabel('Frames')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title('Emotions Probabilities')\n",
    "    ax.legend(fontsize='x-small', loc='upper left')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path, backbone_model='0_66_49_wo_gl', LSTM_model='RAVDESS', device=device):\n",
    "\n",
    "    pth_backbone_model = torch.jit.load(f'model/Torch/torchscript_model_{backbone_model}.pth').to(device)\n",
    "    pth_backbone_model.eval()\n",
    "\n",
    "    pth_LSTM_model = torch.jit.load(f'model/Torch/{LSTM_model}.pth').to(device)\n",
    "    pth_LSTM_model.eval()\n",
    "\n",
    "    DICT_EMO = {0: 'Neutral', 1: 'Happiness', 2: 'Sadness', 3: 'Surprise', 4: 'Fear', 5: 'Disgust', 6: 'Anger'}\n",
    "    emotion_probs = {emotion: [] for emotion in DICT_EMO.values()}\n",
    "    \n",
    "    output_path_basename = os.path.basename(output_path)\n",
    "    output_log_path = f'logs/{output_path_basename}.csv'\n",
    "\n",
    "    data_list = []\n",
    "    emotion_history = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    combined_writer = cv2.VideoWriter(output_path, fourcc, fps, (screen_shape[0], screen_shape[1]))\n",
    "    \n",
    "    blank_screen = np.ones((screen_shape[1], screen_shape[0], 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    lstm_features = []\n",
    "    emotion_probs = {emotion: [] for emotion in DICT_EMO.values()}\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5.37, 2.95))\n",
    "    \n",
    "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    current_emotion = None\n",
    "    emotion_start_time = None\n",
    "    \n",
    "    try:\n",
    "        for frame_count in tqdm(range(total_frames), desc=\"Processing frames\"):\n",
    "            t1 = time.time()\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            \n",
    "            screen = blank_screen.copy()\n",
    "            \n",
    "            frame_copy = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(frame_copy)\n",
    "            \n",
    "            if results.multi_face_landmarks:\n",
    "                for fl in results.multi_face_landmarks:\n",
    "                    startX, startY, endX, endY = fn.get_box(fl, w, h)\n",
    "                    face_copy = frame_copy[startY:endY, startX:endX]\n",
    "                    face_copy = cv2.cvtColor(face_copy, cv2.COLOR_RGB2BGR)\n",
    "                    \n",
    "                    cur_face = pth_processing(Image.fromarray(frame_copy[startY:endY, startX:endX]))\n",
    "                    features = torch.nn.functional.relu(pth_backbone_model.extract_features(cur_face)).cpu().detach().numpy()\n",
    "                    \n",
    "                    if not lstm_features:\n",
    "                        lstm_features = [features] * 10\n",
    "                    else:\n",
    "                        lstm_features.pop(0)\n",
    "                        lstm_features.append(features)\n",
    "                    \n",
    "                    lstm_f = torch.unsqueeze(torch.from_numpy(np.vstack(lstm_features)), 0).to(device)\n",
    "                    output = pth_LSTM_model(lstm_f).cpu().detach().numpy()\n",
    "                    \n",
    "                    for i, emotion in DICT_EMO.items():\n",
    "                        emotion_probs[emotion].append(output[0, i])\n",
    "                    \n",
    "                    cl = np.argmax(output)\n",
    "                    label = DICT_EMO[cl]\n",
    "                    # frame = fn.annotate(frame, (startX, startY, endX, endY), label)\n",
    "                    \n",
    "                    face_region = face_copy\n",
    "                    screen = place_image_on_screen(screen, face_region, face_shape)\n",
    "                    \n",
    "                    if label != current_emotion:\n",
    "                        current_emotion = label\n",
    "                        emotion_start_time = time.time()\n",
    "                    \n",
    "                    if emotion_start_time is not None:\n",
    "                        emotion_duration = time.time() - emotion_start_time\n",
    "                    else:\n",
    "                        emotion_duration = 0\n",
    "                    \n",
    "                    emotion_history.append(label)\n",
    "                    \n",
    "                    predominant_emotion = max(set(emotion_history), key=emotion_history.count)\n",
    "                    current_emotion_prob = output[0, cl]\n",
    "                    \n",
    "                    data_list.append({\n",
    "                        'Frame': frame_count,\n",
    "                        'Emotion': current_emotion,\n",
    "                        'Time': emotion_duration,\n",
    "                        'NE_PROB': output[0, 0],\n",
    "                        'HA_PROB': output[0, 1],\n",
    "                        'SA_PROB': output[0, 2],\n",
    "                        'SU_PROB': output[0, 3],\n",
    "                        'FE_PROB': output[0, 4],\n",
    "                        'DI_PROB': output[0, 5],\n",
    "                        'AN_PROB': output[0, 6]\n",
    "                    })\n",
    "\n",
    "            screen = place_image_on_screen(screen, frame, video_shape)\n",
    "            \n",
    "            update_emotion_graph(ax, emotion_probs)\n",
    "            \n",
    "            fig.canvas.draw()\n",
    "            plot_frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            plot_frame = plot_frame.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plot_frame = cv2.cvtColor(plot_frame, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            screen = place_image_on_screen(screen, plot_frame, graphic_shape)\n",
    "            \n",
    "            fps = 1 / (time.time() - t1)\n",
    "            screen = update_screen_info(screen, predominant_emotion, emotion_duration, current_emotion, current_emotion_prob, backbone_model, LSTM_model, device, fps)\n",
    "            \n",
    "            combined_writer.write(screen)\n",
    "    \n",
    "    finally:\n",
    "        cap.release()\n",
    "        combined_writer.release()\n",
    "        face_mesh.close()\n",
    "        plt.close(fig)\n",
    "\n",
    "        df = pd.DataFrame(data_list)\n",
    "        df.to_csv(output_log_path, index=False)\n",
    "    \n",
    "    return screen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723818468.651249 14616658 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "Processing frames:   0%|          | 0/528 [00:00<?, ?it/s]W0000 00:00:1723818468.654101 14630069 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723818468.657114 14630070 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/77/x5bdxswj42g15hjcw4d35qk40000gn/T/ipykernel_49133/3589553461.py:117: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed in 3.10. Use buffer_rgba instead.\n",
      "  plot_frame = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
      "Processing frames:   0%|          | 1/528 [00:00<05:24,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   1%|          | 3/528 [00:01<03:01,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   1%|          | 4/528 [00:01<02:23,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   1%|          | 6/528 [00:01<01:57,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   2%|▏         | 8/528 [00:02<01:39,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   2%|▏         | 10/528 [00:02<01:34,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   2%|▏         | 12/528 [00:02<01:29,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   3%|▎         | 14/528 [00:03<01:28,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   3%|▎         | 16/528 [00:03<01:25,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   3%|▎         | 17/528 [00:03<01:27,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   4%|▎         | 19/528 [00:03<01:30,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   4%|▍         | 21/528 [00:04<01:26,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   4%|▍         | 23/528 [00:04<01:28,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   5%|▍         | 25/528 [00:04<01:26,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   5%|▌         | 27/528 [00:05<01:22,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   5%|▌         | 29/528 [00:05<01:22,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   6%|▌         | 31/528 [00:06<01:26,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   6%|▋         | 33/528 [00:06<01:23,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   7%|▋         | 35/528 [00:06<01:23,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   7%|▋         | 37/528 [00:07<01:23,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   7%|▋         | 39/528 [00:07<01:20,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   8%|▊         | 41/528 [00:07<01:16,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   8%|▊         | 43/528 [00:07<01:21,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   9%|▊         | 45/528 [00:08<01:20,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   9%|▉         | 47/528 [00:08<01:19,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:   9%|▉         | 49/528 [00:08<01:16,  6.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  10%|▉         | 51/528 [00:09<01:14,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames:  10%|▉         | 51/528 [00:09<01:27,  5.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m video_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(video_src)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLSTM_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m final_screen \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackbone_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLSTM_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLSTM_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m     24\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(final_screen, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
      "Cell \u001b[0;32mIn[20], line 114\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path, output_path, backbone_model, LSTM_model, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m         data_list\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m: frame_count,\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion\u001b[39m\u001b[38;5;124m'\u001b[39m: current_emotion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAN_PROB\u001b[39m\u001b[38;5;124m'\u001b[39m: output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m    110\u001b[0m         })\n\u001b[1;32m    112\u001b[0m screen \u001b[38;5;241m=\u001b[39m place_image_on_screen(screen, frame, video_shape)\n\u001b[0;32m--> 114\u001b[0m \u001b[43mupdate_emotion_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotion_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[1;32m    117\u001b[0m plot_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mtostring_rgb(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m, in \u001b[0;36mupdate_emotion_graph\u001b[0;34m(ax, emotion_probs)\u001b[0m\n\u001b[1;32m     17\u001b[0m ax\u001b[38;5;241m.\u001b[39mlegend(fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-small\u001b[39m\u001b[38;5;124m'\u001b[39m, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper left\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m ax\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtight_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m1.05\u001b[39m)\n\u001b[1;32m     22\u001b[0m ax\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_major_locator(plt\u001b[38;5;241m.\u001b[39mMaxNLocator(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/pyplot.py:2715\u001b[0m, in \u001b[0;36mtight_layout\u001b[0;34m(pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mtight_layout)\n\u001b[1;32m   2708\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtight_layout\u001b[39m(\n\u001b[1;32m   2709\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2713\u001b[0m     rect: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2714\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2715\u001b[0m     \u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtight_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrect\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/figure.py:3538\u001b[0m, in \u001b[0;36mFigure.tight_layout\u001b[0;34m(self, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   3536\u001b[0m previous_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_layout_engine()\n\u001b[1;32m   3537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_layout_engine(engine)\n\u001b[0;32m-> 3538\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3540\u001b[0m     previous_engine, (TightLayoutEngine, PlaceHolderLayoutEngine)\n\u001b[1;32m   3541\u001b[0m ):\n\u001b[1;32m   3542\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe figure layout has changed to tight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/layout_engine.py:183\u001b[0m, in \u001b[0;36mTightLayoutEngine.execute\u001b[0;34m(self, fig)\u001b[0m\n\u001b[1;32m    181\u001b[0m renderer \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m--> 183\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mget_tight_layout_figure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_subplotspec_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh_pad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_pad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    188\u001b[0m     fig\u001b[38;5;241m.\u001b[39msubplots_adjust(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/_tight_layout.py:266\u001b[0m, in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m    262\u001b[0m     span_pairs\u001b[38;5;241m.\u001b[39mappend((\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mslice\u001b[39m(ss\u001b[38;5;241m.\u001b[39mrowspan\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m div_row, ss\u001b[38;5;241m.\u001b[39mrowspan\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m*\u001b[39m div_row),\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28mslice\u001b[39m(ss\u001b[38;5;241m.\u001b[39mcolspan\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m div_col, ss\u001b[38;5;241m.\u001b[39mcolspan\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m*\u001b[39m div_col)))\n\u001b[0;32m--> 266\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_auto_adjust_subplotpars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_nrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ncols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspan_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msubplot_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubplot_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43max_bbox_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max_bbox_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_pad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# kwargs can be none if tight_layout fails...\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# if rect is given, the whole subplots area (including\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# labels) will fit into the rect instead of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# auto_adjust_subplotpars twice, where the second run\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# with adjusted rect parameters.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/_tight_layout.py:82\u001b[0m, in \u001b[0;36m_auto_adjust_subplotpars\u001b[0;34m(fig, renderer, shape, span_pairs, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m subplots:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[0;32m---> 82\u001b[0m         bb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mmartist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tightbbox_for_layout_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     84\u001b[0m tight_bbox_raw \u001b[38;5;241m=\u001b[39m Bbox\u001b[38;5;241m.\u001b[39munion(bb)\n\u001b[1;32m     85\u001b[0m tight_bbox \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39mtransFigure\u001b[38;5;241m.\u001b[39minverted()\u001b[38;5;241m.\u001b[39mtransform_bbox(tight_bbox_raw)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/artist.py:1408\u001b[0m, in \u001b[0;36m_get_tightbbox_for_layout_only\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;124;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfor_layout_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mget_tightbbox(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:457\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    452\u001b[0m     warn_deprecated(\n\u001b[1;32m    453\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    456\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/axes/_base.py:4451\u001b[0m, in \u001b[0;36m_AxesBase.get_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4449\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[1;32m   4450\u001b[0m             bb\u001b[38;5;241m.\u001b[39mappend(ba)\n\u001b[0;32m-> 4451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_title_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4452\u001b[0m axbbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   4453\u001b[0m bb\u001b[38;5;241m.\u001b[39mappend(axbbox)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/axes/_base.py:3027\u001b[0m, in \u001b[0;36m_AxesBase._update_title_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3025\u001b[0m     _log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop of Axes not in the figure, so title not moved\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtitle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mymin \u001b[38;5;241m<\u001b[39m top:\n\u001b[1;32m   3028\u001b[0m     _, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransAxes\u001b[38;5;241m.\u001b[39minverted()\u001b[38;5;241m.\u001b[39mtransform((\u001b[38;5;241m0\u001b[39m, top))\n\u001b[1;32m   3029\u001b[0m     title\u001b[38;5;241m.\u001b[39mset_position((x, y))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/text.py:944\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m    943\u001b[0m         tx, ty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_xy_display()\n\u001b[0;32m--> 944\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m renderer\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/transforms.py:812\u001b[0m, in \u001b[0;36mBbox.from_bounds\u001b[0;34m(x0, y0, width, height)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_bounds\u001b[39m(x0, y0, width, height):\n\u001b[1;32m    807\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;124;03m    Create a new `Bbox` from *x0*, *y0*, *width* and *height*.\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m    *width* and *height* may be negative.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_extents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/transforms.py:830\u001b[0m, in \u001b[0;36mBbox.from_extents\u001b[0;34m(minpos, *args)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_extents\u001b[39m(\u001b[38;5;241m*\u001b[39margs, minpos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    816\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;124;03m    Create a new Bbox from *left*, *bottom*, *right* and *top*.\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m        scales where negative bounds result in floating point errors.\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m \u001b[43mBbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m minpos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m         bbox\u001b[38;5;241m.\u001b[39m_minpos[:] \u001b[38;5;241m=\u001b[39m minpos\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/transforms.py:766\u001b[0m, in \u001b[0;36mBbox.__init__\u001b[0;34m(self, points, **kwargs)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, points, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    760\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    points : `~numpy.ndarray`\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m        A (2, 2) array of the form ``[[x0, y0], [x1, y1]]``.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(points, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m points\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvision/lib/python3.10/site-packages/matplotlib/transforms.py:113\u001b[0m, in \u001b[0;36mTransformNode.__init__\u001b[0;34m(self, shorthand_name)\u001b[0m\n\u001b[1;32m    107\u001b[0m pass_through \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03mIf pass_through is True, all ancestors will always be\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03minvalidated, even if 'self' is already invalid.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, shorthand_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ``str(transform)`` when DEBUG=True.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parents \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_available = {\n",
    "    'backbone': ['0_66_37_wo_gl', '0_66_49_wo_gl'],\n",
    "    'LSTM': ['RAVDESS', 'CREMA-D', 'Aff-Wild2', 'SAVEE', 'RAMAS', 'IEMOCAP']\n",
    "}\n",
    "\n",
    "backbone_model = '0_66_49_wo_gl'\n",
    "LSTM_model = 'SAVEE'\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "video_src = 'src/will.mp4'\n",
    "video_name = os.path.basename(video_src).split('.')[0]\n",
    "output_path = f'output/{backbone_model}_{LSTM_model}_{video_name}.mp4'\n",
    "final_screen = process_video(\n",
    "    video_path=video_src, \n",
    "    output_path=output_path, \n",
    "    backbone_model=backbone_model, \n",
    "    LSTM_model=LSTM_model, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.imshow(cv2.cvtColor(final_screen, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f'Output video saved at {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
